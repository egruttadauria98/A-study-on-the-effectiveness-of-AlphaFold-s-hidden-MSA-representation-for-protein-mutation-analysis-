{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Openfold Colab adaptation\n",
    "\n",
    "This notebook is a modification of the OpenFold Colab notebook: https://colab.research.google.com/github/aqlaboratory/openfold/blob/main/notebooks/OpenFold.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#sys.path.append('./openfold_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "# A filthy hack to avoid slow Linear layer initialization\n",
    "import openfold.model.primitives\n",
    "\n",
    "def __default_linear_init__(self, *args, **kwargs):\n",
    "    return torch.nn.Linear.__init__(\n",
    "      self, \n",
    "      *args[:2], \n",
    "      **{k:v for k,v in kwargs.items() if k == \"bias\"}\n",
    "    )\n",
    "\n",
    "openfold.model.primitives.Linear.__init__ = __default_linear_init__\n",
    "\n",
    "from openfold import config\n",
    "from openfold.data import feature_pipeline\n",
    "from openfold.data import data_pipeline\n",
    "from openfold.model import model\n",
    "from openfold.utils.import_weights import import_jax_weights_\n",
    "from openfold.utils.tensor_utils import tensor_tree_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _placeholder_template_feats(num_templates_, num_res_):\n",
    "  return {\n",
    "      'template_aatype': torch.zeros(num_templates_, num_res_, 22).long(),\n",
    "      'template_all_atom_positions': torch.zeros(num_templates_, num_res_, 37, 3),\n",
    "      'template_all_atom_mask': torch.zeros(num_templates_, num_res_, 37),\n",
    "      'template_domain_names': torch.zeros(num_templates_),\n",
    "      'template_sum_probs': torch.zeros(num_templates_, 1),\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"../data_pickle/deletion_matrices.pickle\", 'rb') as f:\n",
    "    deletion_matrices = pickle.load(f)\n",
    "\n",
    "with open(\"../data_pickle/msas.pickle\", 'rb') as f:\n",
    "    msas = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"MSIQHFRVALIPFFAAFCLPVFAHPETLVKVKDAEDQLGARVGYIELDLNSGKILESFRPEERFPMMSTFKVLLCGAVLSRVDAGQEQLGRRIHYSQNDLVEYSPVTEKHLTDGMTVRELCSAAITMSDNTAANLLLTTIGGPKELTAFLHNMGDHVTRLDRWEPELNEAIPNDERDTTMPAAMATTLRKLLTGELLTLASRQQLIDWMEADKVAGPLLRSALPAGWFIADKSGAGERGSRGIIAALGPDGKPSRIVVIYTTGSQATMDERNRQIAEIGASLIKHW\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"model_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_templates = 1 # dummy number --- is ignored\n",
    "num_res = len(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dict = {}\n",
    "feature_dict.update(data_pipeline.make_sequence_features(sequence, 'test', num_res))\n",
    "feature_dict.update(data_pipeline.make_msa_features(msas, deletion_matrices=deletion_matrices))\n",
    "feature_dict.update(_placeholder_template_feats(num_templates, num_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filepath = \"msa_arr.pickle\"\n",
    "output_dir = \"./../data_pickle\"\n",
    "msa_output_path = os.path.join(output_dir, filepath)\n",
    "with open(msa_output_path, 'wb') as f:\n",
    "    pickle.dump(feature_dict['msa'], f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_set = \"OpenFold\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = config.model_config(model_name)\n",
    "openfold_model = model.AlphaFold(cfg)\n",
    "openfold_model = openfold_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENFOLD_PARAMS_DIR = './openfold/resources/openfold_params'\n",
    "ALPHAFOLD_PARAMS_DIR = './openfold/openfold/resources/params'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(weight_set == \"AlphaFold\"):\n",
    "  params_name = os.path.join(ALPHAFOLD_PARAMS_DIR, f\"params_{model_name}.npz\")\n",
    "  import_jax_weights_(openfold_model, params_name, version=model_name)\n",
    "\n",
    "elif(weight_set == \"OpenFold\"):\n",
    "  model_name_spl = model_name.split(\"_\")\n",
    "\n",
    "  if(model_name_spl[-1] == \"ptm\"):\n",
    "    of_model_name = \"finetuning_ptm_2.pt\"\n",
    "    \n",
    "  else:\n",
    "    of_model_name = f\"finetuning_{model_name_spl[-1]}.pt\"\n",
    "    \n",
    "  params_name = os.path.join(\n",
    "    OPENFOLD_PARAMS_DIR,\n",
    "    of_model_name\n",
    "  )\n",
    "\n",
    "  d = torch.load(params_name)\n",
    "  openfold_model.load_state_dict(d)\n",
    "else:\n",
    "  raise ValueError(f\"Invalid weight set: {weight_set}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "openfold_model = openfold_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elio/Desktop/openfold_deepsequence_integration_pytorch/openfold_model/openfold/data/feature_pipeline.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  k: torch.tensor(v) for k, v in np_example.items() if k in features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tRecycling from configs\n",
      "\tCALLING MSA SAMPLING\n",
      "Dumping pickle at ./../data_pickle/sel_seq.pickle\n",
      "Dumping pickle at ./../data_pickle/not_sel_seq.pickle\n"
     ]
    }
   ],
   "source": [
    "pipeline = feature_pipeline.FeaturePipeline(cfg.data)\n",
    " \n",
    "processed_feature_dict = pipeline.process_features(\n",
    "  feature_dict, mode='predict'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['aatype', 'residue_index', 'seq_length', 'template_aatype', 'template_all_atom_positions', 'template_all_atom_mask', 'template_sum_probs', 'seq_mask', 'msa_mask', 'msa_row_mask', 'template_mask', 'template_pseudo_beta', 'template_pseudo_beta_mask', 'template_torsion_angles_sin_cos', 'template_alt_torsion_angles_sin_cos', 'template_torsion_angles_mask', 'atom14_atom_exists', 'residx_atom14_to_atom37', 'residx_atom37_to_atom14', 'atom37_atom_exists', 'extra_msa', 'extra_msa_mask', 'extra_msa_row_mask', 'bert_mask', 'true_msa', 'extra_has_deletion', 'extra_deletion_value', 'msa_feat', 'target_feat'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_feature_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 286, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_feature_dict['true_msa'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_feature_dict = tensor_tree_map(\n",
    "    lambda t: t.cuda(), processed_feature_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 286, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_feature_dict['true_msa'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "with torch.no_grad():\n",
    "  prediction_result = openfold_model(processed_feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['msa', 'pair', 'single', 'sm', 'final_atom_positions', 'final_atom_mask', 'final_affine_tensor', 'lddt_logits', 'plddt', 'distogram_logits', 'masked_msa_logits', 'experimentally_resolved_logits'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_result['msa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 286, 256])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_result['msa'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filepath = \"prediction_result.pickle\"\n",
    "output_dir = \"./../data_pickle\"\n",
    "msa_output_path = os.path.join(output_dir, filepath)\n",
    "with open(msa_output_path, 'wb') as f:\n",
    "    pickle.dump(prediction_result, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b734eb7758abf3393a931433e51bb29d8c11a2cadf78a044dad7bdf5c07010d3"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
