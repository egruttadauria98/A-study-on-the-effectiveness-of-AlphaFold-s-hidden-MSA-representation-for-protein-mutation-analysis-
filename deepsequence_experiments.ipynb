{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"./deepsequence_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSA array 3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data_pickle/msa_arr.pickle\", 'rb') as f:\n",
    "    msa_arr = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10813, 286)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msa_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "msa_arr_3d = np.zeros(shape=(msa_arr.shape[0], msa_arr.shape[1], 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx_sequence, sequence in enumerate(msa_arr):\n",
    "    for idx_amino, amino_value in enumerate(sequence):\n",
    "        if amino_value < 20:\n",
    "            msa_arr_3d[idx_sequence, idx_amino, amino_value] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msa_arr_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "filepath = \"msa_arr_3d.pickle\"\n",
    "output_dir = \"./data_pickle\"\n",
    "msa_output_path = os.path.join(output_dir, filepath)\n",
    "with open(msa_output_path, 'wb') as f:\n",
    "    pickle.dump(msa_arr_3d, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import DataHelper\n",
    "\n",
    "class DataHelperAugmented(DataHelper):\n",
    "\n",
    "    def __init__(self,\n",
    "                 data_pickle_path,\n",
    "                 calc_weights=True,\n",
    "                 theta=0.2):\n",
    "\n",
    "        # Pass to the parents the only arguments that can be changed\n",
    "        super().__init__(calc_weights=calc_weights, theta=theta) \n",
    "\n",
    "        # x_train = msa_3d for consistency with parent class\n",
    "        self.x_train = self._unpickle_data(data_pickle_path)\n",
    "        self.seq_len = self.x_train.shape[1]\n",
    "        self.alphabet_size = 20\n",
    "\n",
    "        # Compute weights and Neff\n",
    "        if calc_weights:\n",
    "            self._compute_weights()\n",
    "        else:\n",
    "            self._isotropic_weights()\n",
    "        self.n_eff = np.sum(self.weights)\n",
    "\n",
    "    \n",
    "    def _unpickle_data(self, data_pickle_path):\n",
    "        with open(data_pickle_path, 'rb') as pickleFile:\n",
    "            return pickle.load(pickleFile)\n",
    "\n",
    "    def _compute_weights(self):\n",
    "        '''X = T.tensor3(\"x\")\n",
    "        cutoff = T.scalar(\"theta\")\n",
    "        X_flat = X.reshape((X.shape[0], X.shape[1]*X.shape[2]))\n",
    "        N_list, updates = theano.map(lambda x: 1.0 / T.sum(T.dot(X_flat, x) / T.dot(x, x) > 1 - cutoff), X_flat)\n",
    "        weightfun = theano.function(inputs=[X, cutoff], outputs=[N_list],allow_input_downcast=True)\n",
    "        self.weights = weightfun(self.x_train, self.theta)[0]'''\n",
    "        pass\n",
    "\n",
    "    def _isotropic_weights(self):\n",
    "        self.weights = np.ones(self.x_train.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline DataHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_msa_3d = 'data_pickle/msa_arr_3d.pickle'\n",
    "baseline_data_helper = DataHelperAugmented(data_pickle_path=path_msa_3d, calc_weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10813, 286, 20)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_data_helper.x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import VAE\n",
    "\n",
    "model = VAE(baseline_data_helper, bayesian=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (fc1): Linear(in_features=5720, out_features=1500, bias=True)\n",
       "  (fc2): Linear(in_features=1500, out_features=1500, bias=True)\n",
       "  (fc3_mu): Linear(in_features=1500, out_features=40, bias=True)\n",
       "  (fc3_logvar): Linear(in_features=1500, out_features=40, bias=True)\n",
       "  (bfc4): Linear(in_features=40, out_features=100, bias=False)\n",
       "  (bfc5): Linear(in_features=100, out_features=2000, bias=True)\n",
       "  (bfc6): Linear(in_features=2000, out_features=5720, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Epoch: 0\n",
      "\tMSE loss: 0.25085827708244324\n",
      "\tKL latent loss: 0.01957143284380436\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.2508602440357208\n",
      "Epoch: 100\n",
      "\tMSE loss: 0.023568730801343918\n",
      "\tKL latent loss: 2.823861598968506\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.023851117119193077\n",
      "Epoch: 200\n",
      "\tMSE loss: 0.02263300120830536\n",
      "\tKL latent loss: 5.465763568878174\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.02317957766354084\n",
      "Epoch: 300\n",
      "\tMSE loss: 0.020691299811005592\n",
      "\tKL latent loss: 8.810708045959473\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.02157237008213997\n",
      "Epoch: 400\n",
      "\tMSE loss: 0.01925746724009514\n",
      "\tKL latent loss: 10.130838394165039\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.02027055062353611\n",
      "Epoch: 500\n",
      "\tMSE loss: 0.01954415999352932\n",
      "\tKL latent loss: 11.097723960876465\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.02065393328666687\n",
      "Epoch: 600\n",
      "\tMSE loss: 0.018548965454101562\n",
      "\tKL latent loss: 11.82666015625\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.019731631502509117\n",
      "Epoch: 700\n",
      "\tMSE loss: 0.018983354791998863\n",
      "\tKL latent loss: 12.318801879882812\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.020215235650539398\n",
      "Epoch: 800\n",
      "\tMSE loss: 0.017692748457193375\n",
      "\tKL latent loss: 12.877182006835938\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.018980465829372406\n",
      "Epoch: 900\n",
      "\tMSE loss: 0.01727982796728611\n",
      "\tKL latent loss: 13.207925796508789\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.018600620329380035\n",
      "Epoch: 1000\n",
      "\tMSE loss: 0.01739824377000332\n",
      "\tKL latent loss: 13.961735725402832\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.01879441738128662\n",
      "Epoch: 1100\n",
      "\tMSE loss: 0.016116507351398468\n",
      "\tKL latent loss: 12.939860343933105\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.01741049252450466\n",
      "Epoch: 1200\n",
      "\tMSE loss: 0.017467975616455078\n",
      "\tKL latent loss: 13.005918502807617\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.018768567591905594\n",
      "Epoch: 1300\n",
      "\tMSE loss: 0.015808027237653732\n",
      "\tKL latent loss: 14.029056549072266\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.017210932448506355\n",
      "Epoch: 1400\n",
      "\tMSE loss: 0.016385702416300774\n",
      "\tKL latent loss: 15.006193161010742\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.017886321991682053\n",
      "Epoch: 1500\n",
      "\tMSE loss: 0.015918005257844925\n",
      "\tKL latent loss: 15.443438529968262\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.01746234856545925\n",
      "Epoch: 1600\n",
      "\tMSE loss: 0.015993952751159668\n",
      "\tKL latent loss: 14.500341415405273\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.017443986609578133\n",
      "Epoch: 1700\n",
      "\tMSE loss: 0.014730523340404034\n",
      "\tKL latent loss: 16.33268928527832\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.01636379212141037\n",
      "Epoch: 1800\n",
      "\tMSE loss: 0.015577064827084541\n",
      "\tKL latent loss: 15.062173843383789\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.01708328165113926\n",
      "Epoch: 1900\n",
      "\tMSE loss: 0.016372045502066612\n",
      "\tKL latent loss: 15.56529712677002\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.017928574234247208\n",
      "Epoch: 2000\n",
      "\tMSE loss: 0.01526630762964487\n",
      "\tKL latent loss: 15.3073091506958\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.016797037795186043\n",
      "Epoch: 2100\n",
      "\tMSE loss: 0.015222601592540741\n",
      "\tKL latent loss: 16.139060974121094\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.016836507245898247\n",
      "Epoch: 2200\n",
      "\tMSE loss: 0.015195329673588276\n",
      "\tKL latent loss: 15.594758987426758\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.01675480604171753\n",
      "Epoch: 2300\n",
      "\tMSE loss: 0.014053176157176495\n",
      "\tKL latent loss: 16.265687942504883\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.015679745003581047\n",
      "Epoch: 2400\n",
      "\tMSE loss: 0.013613301329314709\n",
      "\tKL latent loss: 16.094966888427734\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.015222798101603985\n",
      "Epoch: 2500\n",
      "\tMSE loss: 0.014254645444452763\n",
      "\tKL latent loss: 15.813027381896973\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.015835948288440704\n",
      "Epoch: 2600\n",
      "\tMSE loss: 0.013610766269266605\n",
      "\tKL latent loss: 16.753599166870117\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.015286126174032688\n",
      "Epoch: 2700\n",
      "\tMSE loss: 0.013041275553405285\n",
      "\tKL latent loss: 16.738014221191406\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.014715077355504036\n",
      "Epoch: 2800\n",
      "\tMSE loss: 0.013393404893577099\n",
      "\tKL latent loss: 17.21131706237793\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.015114536508917809\n",
      "Epoch: 2900\n",
      "\tMSE loss: 0.013450007885694504\n",
      "\tKL latent loss: 17.916887283325195\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.015241696499288082\n"
     ]
    }
   ],
   "source": [
    "from train import train\n",
    "\n",
    "\n",
    "model_trained = train(baseline_data_helper, \n",
    "                      copy.deepcopy(model), \n",
    "                      kl_latent_scale=0.0001,\n",
    "                      kl_weights_scale=0,  \n",
    "                      lr=0.001,\n",
    "                      num_updates=3000,\n",
    "                      batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning with MSA representation\n",
    "\n",
    "OpenFold is trained by giving as an input the MSA of the protein family (which is the input of the baseline model as well). <br>\n",
    "The dimension of the MSA representation is smaller that the original MSA because only 508 sequences are sampled. <br>\n",
    "The indices of the original MSA that have been sampled to re-produce the MSA representation are contained in sel_seq.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open output from OpenFold and extract the MSA representation\n",
    "\n",
    "with open(\"data_pickle/prediction_result/prediction_result.pickle\", \"rb\") as file:\n",
    "    af_repr1 = pickle.load(file)['msa'].cpu().numpy()\n",
    "\n",
    "with open(\"data_pickle/prediction_result/prediction_result_2.pickle\", \"rb\") as file:\n",
    "    af_repr2 = pickle.load(file)['msa'].cpu().numpy()\n",
    "\n",
    "with open(\"data_pickle/prediction_result/prediction_result_3.pickle\", \"rb\") as file:\n",
    "    af_repr3 = pickle.load(file)['msa'].cpu().numpy()\n",
    "\n",
    "with open(\"data_pickle/prediction_result/prediction_result_4.pickle\", \"rb\") as file:\n",
    "    af_repr4 = pickle.load(file)['msa'].cpu().numpy()\n",
    "\n",
    "with open(\"data_pickle/prediction_result/prediction_result_5.pickle\", \"rb\") as file:\n",
    "    af_repr5 = pickle.load(file)['msa'].cpu().numpy()\n",
    "\n",
    "with open(\"data_pickle/prediction_result/prediction_result_6.pickle\", \"rb\") as file:\n",
    "    af_repr6 = pickle.load(file)['msa'].cpu().numpy()\n",
    "\n",
    "with open(\"data_pickle/prediction_result/prediction_result_7.pickle\", \"rb\") as file:\n",
    "    af_repr7 = pickle.load(file)['msa'].cpu().numpy()\n",
    "\n",
    "with open(\"data_pickle/prediction_result/prediction_result_8.pickle\", \"rb\") as file:\n",
    "    af_repr8 = pickle.load(file)['msa'].cpu().numpy()\n",
    "\n",
    "with open(\"data_pickle/prediction_result/prediction_result_9.pickle\", \"rb\") as file:\n",
    "    af_repr9 = pickle.load(file)['msa'].cpu().numpy()\n",
    "\n",
    "with open(\"data_pickle/prediction_result/prediction_result_10.pickle\", \"rb\") as file:\n",
    "    af_repr10 = pickle.load(file)['msa'].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5120, 286, 256)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "af_repr = np.stack((af_repr1, af_repr2, af_repr3, af_repr4, af_repr5, af_repr6, af_repr7, af_repr8, af_repr9, af_repr10), axis=0).reshape(10*512, 286, 256)\n",
    "af_repr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check reshape correctness\n",
    "\n",
    "(af_repr[512:512*2] == af_repr2).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open sampled indices from the MSA to make the MSA representation\n",
    "\n",
    "with open(\"data_pickle/sel_seq/sel_seq.pickle\", \"rb\") as file:\n",
    "    sel_seq_1 = pickle.load(file)\n",
    "\n",
    "with open(\"data_pickle/sel_seq/sel_seq_2.pickle\", \"rb\") as file:\n",
    "    sel_seq_2 = pickle.load(file)\n",
    "\n",
    "with open(\"data_pickle/sel_seq/sel_seq_3.pickle\", \"rb\") as file:\n",
    "    sel_seq_3 = pickle.load(file)\n",
    "\n",
    "with open(\"data_pickle/sel_seq/sel_seq_4.pickle\", \"rb\") as file:\n",
    "    sel_seq_4 = pickle.load(file)\n",
    "\n",
    "with open(\"data_pickle/sel_seq/sel_seq_5.pickle\", \"rb\") as file:\n",
    "    sel_seq_5 = pickle.load(file)\n",
    "\n",
    "with open(\"data_pickle/sel_seq/sel_seq_6.pickle\", \"rb\") as file:\n",
    "    sel_seq_6 = pickle.load(file)\n",
    "\n",
    "with open(\"data_pickle/sel_seq/sel_seq_7.pickle\", \"rb\") as file:\n",
    "    sel_seq_7 = pickle.load(file)\n",
    "\n",
    "with open(\"data_pickle/sel_seq/sel_seq_8.pickle\", \"rb\") as file:\n",
    "    sel_seq_8 = pickle.load(file)\n",
    "\n",
    "with open(\"data_pickle/sel_seq/sel_seq_9.pickle\", \"rb\") as file:\n",
    "    sel_seq_9 = pickle.load(file)\n",
    "\n",
    "with open(\"data_pickle/sel_seq/sel_seq_10.pickle\", \"rb\") as file:\n",
    "    sel_seq_10 = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5120,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel_seq = np.stack((sel_seq_1, sel_seq_2, sel_seq_3, sel_seq_4, sel_seq_5, sel_seq_6, sel_seq_7, sel_seq_8, sel_seq_9, sel_seq_10)).reshape(512*10)\n",
    "sel_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data_pickle/not_sel_seq.pickle\", \"rb\") as file:\n",
    "    not_sel_seq = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10813"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just to check\n",
    "# The index of the selected and non selected amino acids sum up to the total leght of the MSA\n",
    "\n",
    "len(sel_seq_1) + len(not_sel_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA \n",
    "\n",
    "The MSA representation has 256 channels. In order to reproduce the same shape as the normal MSA, we perform PCA on the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the MSA representation as a array 2d, with the challens as second dimension\n",
    "\n",
    "af_repr_2d = af_repr.reshape(-1, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pca.fit_transform(af_repr_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the MSA representation as a 3d array\n",
    "\n",
    "res_3d = res.reshape(512*10, 286, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 82.41776  ,  -1.7027469,  30.132208 ,   2.3998735,  17.69152  ,\n",
       "         8.770821 ,  -2.0832384,  18.866558 ,  15.341218 , -14.330871 ,\n",
       "       -22.691286 ,  -9.659728 , -15.043725 ,  -5.754189 ,  27.090532 ,\n",
       "       -16.156355 , -13.270078 ,  19.071564 , -29.013563 ,  42.09233  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_3d[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min-Max scaling\n",
    "\n",
    "Scale the distribution between 0 and 1 like the MSA one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_3d_scaled = np.zeros(res_3d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, matr_2d in enumerate(res_3d):\n",
    "    for j, row in enumerate(matr_2d):\n",
    "\n",
    "        vect = row\n",
    "        vect = vect - min(vect)\n",
    "        vect = vect/max(vect)\n",
    "\n",
    "        res_3d_scaled[i, j] = vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.24509101, 0.53078222, 0.28190848, 0.41913784,\n",
       "       0.33908224, 0.24167642, 0.42968276, 0.3980459 , 0.1317645 ,\n",
       "       0.05673698, 0.17368397, 0.12536724, 0.20873281, 0.50348586,\n",
       "       0.11538235, 0.1412842 , 0.43152252, 0.        , 0.63811404])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All the number are between 0 and 1\n",
    "# The index with the highest value now is 1\n",
    "\n",
    "res_3d_scaled[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning DataHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_data_helper = DataHelperAugmented(data_pickle_path=path_msa_3d, calc_weights=False)\n",
    "\n",
    "# x_train_finetune holds the MSA representation as the input for the finetune of the model\n",
    "finetune_data_helper.x_train_finetune = res_3d_scaled\n",
    "\n",
    "# x_train holds the output of the VAE, \n",
    "# therefore during finetuning the model learns to reconstruct the MSA one-hot from the MSA representation\n",
    "# IMPORTANT: the MSA one-hot is sliced with sel_seq, which contains the indices sampled to obtain the MSA representation\n",
    "finetune_data_helper.x_train = finetune_data_helper.x_train[sel_seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Epoch: 0\n",
      "\tMSE loss: 0.04015533626079559\n",
      "\tKL latent loss: 132.97535705566406\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.05345287173986435\n",
      "Epoch: 100\n",
      "\tMSE loss: 0.018478505313396454\n",
      "\tKL latent loss: 11.365095138549805\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.019615015015006065\n",
      "Epoch: 200\n",
      "\tMSE loss: 0.014805679209530354\n",
      "\tKL latent loss: 15.02550983428955\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.01630822941660881\n",
      "Epoch: 300\n",
      "\tMSE loss: 0.01240499783307314\n",
      "\tKL latent loss: 18.77210235595703\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.014282207936048508\n",
      "Epoch: 400\n",
      "\tMSE loss: 0.010501372627913952\n",
      "\tKL latent loss: 20.828603744506836\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.012584232725203037\n",
      "Epoch: 500\n",
      "\tMSE loss: 0.008410325273871422\n",
      "\tKL latent loss: 22.045391082763672\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.010614864528179169\n",
      "Epoch: 600\n",
      "\tMSE loss: 0.007220648694783449\n",
      "\tKL latent loss: 23.404272079467773\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.009561075828969479\n",
      "Epoch: 700\n",
      "\tMSE loss: 0.004951528739184141\n",
      "\tKL latent loss: 23.91754150390625\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.007343282923102379\n",
      "Epoch: 800\n",
      "\tMSE loss: 0.0044388119131326675\n",
      "\tKL latent loss: 24.067855834960938\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.006845597177743912\n",
      "Epoch: 900\n",
      "\tMSE loss: 0.004225568845868111\n",
      "\tKL latent loss: 23.722919464111328\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.006597860716283321\n"
     ]
    }
   ],
   "source": [
    "from train import train\n",
    "\n",
    "model_finetune = train(finetune_data_helper, \n",
    "                       copy.deepcopy(model_trained),\n",
    "                       finetune=True, \n",
    "                       kl_latent_scale=0.0001,\n",
    "                       kl_weights_scale=0,  \n",
    "                       lr=0.001,\n",
    "                       num_updates=1000,\n",
    "                       batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Epoch: 0\n",
      "\tMSE loss: 0.0031853094696998596\n",
      "\tKL latent loss: 23.8128604888916\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.005566595122218132\n",
      "Epoch: 100\n",
      "\tMSE loss: 0.002699552569538355\n",
      "\tKL latent loss: 22.800432205200195\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.0049795955419540405\n",
      "Epoch: 200\n",
      "\tMSE loss: 0.002934566466137767\n",
      "\tKL latent loss: 22.27725601196289\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.005162292160093784\n",
      "Epoch: 300\n",
      "\tMSE loss: 0.0024571844842284918\n",
      "\tKL latent loss: 22.631139755249023\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.004720298573374748\n",
      "Epoch: 400\n",
      "\tMSE loss: 0.00217860727570951\n",
      "\tKL latent loss: 22.12660026550293\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.004391266964375973\n",
      "Epoch: 500\n",
      "\tMSE loss: 0.002409042092040181\n",
      "\tKL latent loss: 22.482479095458984\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.004657289944589138\n",
      "Epoch: 600\n",
      "\tMSE loss: 0.0026889117434620857\n",
      "\tKL latent loss: 22.1121768951416\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.0049001295119524\n",
      "Epoch: 700\n",
      "\tMSE loss: 0.0023127186577767134\n",
      "\tKL latent loss: 22.228872299194336\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.0045356061309576035\n",
      "Epoch: 800\n",
      "\tMSE loss: 0.0023608612827956676\n",
      "\tKL latent loss: 22.32781410217285\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.004593642428517342\n",
      "Epoch: 900\n",
      "\tMSE loss: 0.0026703879702836275\n",
      "\tKL latent loss: 22.540111541748047\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.004924398846924305\n"
     ]
    }
   ],
   "source": [
    "# Lower learning rate\n",
    "\n",
    "model_finetune2 = train(finetune_data_helper, \n",
    "                       copy.deepcopy(model_finetune),\n",
    "                       finetune=True, \n",
    "                       kl_latent_scale=0.0001,\n",
    "                       kl_weights_scale=0,  \n",
    "                       lr=0.0001,\n",
    "                       batch_size=100,\n",
    "                       num_updates=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainig from scratch with AF representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Epoch: 0\n",
      "\tMSE loss: 0.25083133578300476\n",
      "\tKL latent loss: 0.09518483281135559\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.25084084272384644\n",
      "Epoch: 100\n",
      "\tMSE loss: 0.02280583791434765\n",
      "\tKL latent loss: 2.9082722663879395\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.023096665740013123\n",
      "Epoch: 200\n",
      "\tMSE loss: 0.021986011415719986\n",
      "\tKL latent loss: 5.091205596923828\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.022495131939649582\n",
      "Epoch: 300\n",
      "\tMSE loss: 0.021461665630340576\n",
      "\tKL latent loss: 7.8939361572265625\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.022251058369874954\n",
      "Epoch: 400\n",
      "\tMSE loss: 0.0214969664812088\n",
      "\tKL latent loss: 6.69047737121582\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.022166013717651367\n",
      "Epoch: 500\n",
      "\tMSE loss: 0.01927795447409153\n",
      "\tKL latent loss: 10.101778030395508\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.0202881321310997\n",
      "Epoch: 600\n",
      "\tMSE loss: 0.018945792689919472\n",
      "\tKL latent loss: 9.728129386901855\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.01991860568523407\n",
      "Epoch: 700\n",
      "\tMSE loss: 0.017971545457839966\n",
      "\tKL latent loss: 11.45556640625\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.019117102026939392\n",
      "Epoch: 800\n",
      "\tMSE loss: 0.017210304737091064\n",
      "\tKL latent loss: 10.296032905578613\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.018239907920360565\n",
      "Epoch: 900\n",
      "\tMSE loss: 0.016553549095988274\n",
      "\tKL latent loss: 10.837519645690918\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.017637301236391068\n",
      "Epoch: 1000\n",
      "\tMSE loss: 0.016642626374959946\n",
      "\tKL latent loss: 12.832311630249023\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.01792585663497448\n",
      "Epoch: 1100\n",
      "\tMSE loss: 0.015596942976117134\n",
      "\tKL latent loss: 12.261402130126953\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.01682308316230774\n",
      "Epoch: 1200\n",
      "\tMSE loss: 0.014709134586155415\n",
      "\tKL latent loss: 14.951756477355957\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.016204310581088066\n",
      "Epoch: 1300\n",
      "\tMSE loss: 0.014308668673038483\n",
      "\tKL latent loss: 15.081125259399414\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.015816781669855118\n",
      "Epoch: 1400\n",
      "\tMSE loss: 0.012049711309373379\n",
      "\tKL latent loss: 18.03094482421875\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.013852805830538273\n",
      "Epoch: 1500\n",
      "\tMSE loss: 0.011128149926662445\n",
      "\tKL latent loss: 18.353242874145508\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.012963473796844482\n",
      "Epoch: 1600\n",
      "\tMSE loss: 0.01198463886976242\n",
      "\tKL latent loss: 17.741344451904297\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.013758772984147072\n",
      "Epoch: 1700\n",
      "\tMSE loss: 0.009786487556993961\n",
      "\tKL latent loss: 20.731170654296875\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.011859605088829994\n",
      "Epoch: 1800\n",
      "\tMSE loss: 0.009793000295758247\n",
      "\tKL latent loss: 21.164840698242188\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.01190948486328125\n",
      "Epoch: 1900\n",
      "\tMSE loss: 0.007798679638653994\n",
      "\tKL latent loss: 20.827165603637695\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.009881395846605301\n",
      "Epoch: 2000\n",
      "\tMSE loss: 0.007501270156353712\n",
      "\tKL latent loss: 22.392587661743164\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.009740528650581837\n",
      "Epoch: 2100\n",
      "\tMSE loss: 0.006100361235439777\n",
      "\tKL latent loss: 21.486539840698242\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.008249014616012573\n",
      "Epoch: 2200\n",
      "\tMSE loss: 0.005734608508646488\n",
      "\tKL latent loss: 20.97425651550293\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.007832033559679985\n",
      "Epoch: 2300\n",
      "\tMSE loss: 0.0051683769561350346\n",
      "\tKL latent loss: 21.080604553222656\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.007276437245309353\n",
      "Epoch: 2400\n",
      "\tMSE loss: 0.005348970182240009\n",
      "\tKL latent loss: 20.662710189819336\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.0074152410961687565\n",
      "Epoch: 2500\n",
      "\tMSE loss: 0.0038734569679945707\n",
      "\tKL latent loss: 20.78809928894043\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.005952266976237297\n",
      "Epoch: 2600\n",
      "\tMSE loss: 0.004487131256610155\n",
      "\tKL latent loss: 21.668743133544922\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.0066540054976940155\n",
      "Epoch: 2700\n",
      "\tMSE loss: 0.0037134489975869656\n",
      "\tKL latent loss: 19.779918670654297\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.005691440775990486\n",
      "Epoch: 2800\n",
      "\tMSE loss: 0.0032199821434915066\n",
      "\tKL latent loss: 19.987285614013672\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.005218710750341415\n",
      "Epoch: 2900\n",
      "\tMSE loss: 0.0035067726857960224\n",
      "\tKL latent loss: 20.23228645324707\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.005530001595616341\n"
     ]
    }
   ],
   "source": [
    "model_finetune_scratch = train(finetune_data_helper, \n",
    "                              copy.deepcopy(model),\n",
    "                              finetune=True, \n",
    "                              kl_latent_scale=0.0001,\n",
    "                              kl_weights_scale=0,  \n",
    "                              lr=0.001,\n",
    "                              batch_size=100,\n",
    "                              num_updates=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Epoch: 0\n",
      "\tMSE loss: 0.003354511922225356\n",
      "\tKL latent loss: 20.56233787536621\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.005410745739936829\n",
      "Epoch: 100\n",
      "\tMSE loss: 0.002729611238464713\n",
      "\tKL latent loss: 18.48223304748535\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.004577834624797106\n",
      "Epoch: 200\n",
      "\tMSE loss: 0.0028627924621105194\n",
      "\tKL latent loss: 18.56342887878418\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.0047191353514790535\n",
      "Epoch: 300\n",
      "\tMSE loss: 0.0030346328858286142\n",
      "\tKL latent loss: 18.31036949157715\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.004865669645369053\n",
      "Epoch: 400\n",
      "\tMSE loss: 0.00245704990811646\n",
      "\tKL latent loss: 18.10809326171875\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.004267859272658825\n",
      "Epoch: 500\n",
      "\tMSE loss: 0.0027011772617697716\n",
      "\tKL latent loss: 18.410919189453125\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.00454226927831769\n",
      "Epoch: 600\n",
      "\tMSE loss: 0.002803846262395382\n",
      "\tKL latent loss: 17.823972702026367\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.004586243536323309\n",
      "Epoch: 700\n",
      "\tMSE loss: 0.002847309224307537\n",
      "\tKL latent loss: 18.553428649902344\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.004702651873230934\n",
      "Epoch: 800\n",
      "\tMSE loss: 0.002995644463226199\n",
      "\tKL latent loss: 18.127391815185547\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.0048083835281431675\n",
      "Epoch: 900\n",
      "\tMSE loss: 0.002487536985427141\n",
      "\tKL latent loss: 18.193153381347656\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.004306852351874113\n"
     ]
    }
   ],
   "source": [
    "# Lower learning rate\n",
    "\n",
    "model_finetune_scratch2 = train(finetune_data_helper, \n",
    "                              copy.deepcopy(model_finetune_scratch),\n",
    "                              finetune=True, \n",
    "                              kl_latent_scale=0.0001,\n",
    "                              kl_weights_scale=0,  \n",
    "                              lr=0.0001,\n",
    "                              batch_size=100,\n",
    "                              num_updates=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Epoch: 0\n",
      "\tMSE loss: 0.0027777275536209345\n",
      "\tKL latent loss: 18.18413734436035\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.004596141166985035\n",
      "Epoch: 100\n",
      "\tMSE loss: 0.0027726776897907257\n",
      "\tKL latent loss: 18.381969451904297\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.004610874690115452\n",
      "Epoch: 200\n",
      "\tMSE loss: 0.002794278785586357\n",
      "\tKL latent loss: 17.877988815307617\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.004582077730447054\n",
      "Epoch: 300\n",
      "\tMSE loss: 0.0026489070151001215\n",
      "\tKL latent loss: 17.560789108276367\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.004404985811561346\n",
      "Epoch: 400\n",
      "\tMSE loss: 0.0025657638907432556\n",
      "\tKL latent loss: 17.6796817779541\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.004333732184022665\n",
      "Epoch: 500\n",
      "\tMSE loss: 0.0027674755547195673\n",
      "\tKL latent loss: 17.665796279907227\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.004534055013209581\n",
      "Epoch: 600\n",
      "\tMSE loss: 0.0024628043174743652\n",
      "\tKL latent loss: 17.439754486083984\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.004206779878586531\n",
      "Epoch: 700\n",
      "\tMSE loss: 0.0029619047418236732\n",
      "\tKL latent loss: 18.44240379333496\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.004806145094335079\n",
      "Epoch: 800\n",
      "\tMSE loss: 0.0025698039680719376\n",
      "\tKL latent loss: 17.71453285217285\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.004341257270425558\n",
      "Epoch: 900\n",
      "\tMSE loss: 0.0026967248413711786\n",
      "\tKL latent loss: 17.735450744628906\n",
      "\tKL weights loss: 0\n",
      "\tTotal loss:0.004470270127058029\n"
     ]
    }
   ],
   "source": [
    "# Even lower learning rate\n",
    "\n",
    "model_finetune_scratch3 = train(finetune_data_helper, \n",
    "                              copy.deepcopy(model_finetune_scratch2),\n",
    "                              finetune=True, \n",
    "                              kl_latent_scale=0.0001,\n",
    "                              kl_weights_scale=0,  \n",
    "                              lr=0.00001,\n",
    "                              batch_size=100,\n",
    "                              num_updates=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model\n",
    "\n",
    "The actual accuracy is higher that the one below because many sequences have missing amino acids (-) and in this simple evaluation we predict at each position with the argmax over all 20 amino acids. Therefore each prediction at missing position is always wrong unless 0 by chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_performance_tests import reconstruction_accuracy_per_aminoacid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.965034965034965"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performance randomly initialized model\n",
    "\n",
    "reconstruction_accuracy_per_aminoacid(model, data_helper=baseline_data_helper, n_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59.65734265734269"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performance baseline model\n",
    "\n",
    "reconstruction_accuracy_per_aminoacid(model_trained, data_helper=baseline_data_helper, n_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70.00349650349649"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performance finetuned model - with MSA representation\n",
    "\n",
    "reconstruction_accuracy_per_aminoacid(model_finetune, data_helper=finetune_data_helper, n_samples=100, finetune=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70.38111888111887"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performance finetuned model with lower learning rate - with MSA representation\n",
    "\n",
    "reconstruction_accuracy_per_aminoacid(model_finetune2, data_helper=finetune_data_helper, n_samples=100, finetune=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66.7867132867133"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performance model learning from scratch - with MSA representation\n",
    "\n",
    "reconstruction_accuracy_per_aminoacid(model_finetune_scratch, data_helper=finetune_data_helper, n_samples=100, finetune=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70.05244755244755"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performance model learning from scratch - with MSA representation\n",
    "\n",
    "reconstruction_accuracy_per_aminoacid(model_finetune_scratch3, data_helper=finetune_data_helper, n_samples=100, finetune=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stability experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO BE DONE\n",
    "#baseline_data_helper.delta_elbo(baseline_data_helper,[(175,\"A\",\"C\")], N_pred_iterations=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7560b37db0cbb8bcb003a30ef9ed03000c9426f9b73fc99c7d241b0a108e2716"
  },
  "kernelspec": {
   "display_name": "Python [conda env:deep_sequence_pytorch_env]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
